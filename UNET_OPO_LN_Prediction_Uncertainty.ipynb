{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/mikedoho/PycharmProjects/OPO_LN_Pred_Uncertainty/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/mikedoho/PycharmProjects/OPO_LN_Pred_Uncertainty/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/mikedoho/PycharmProjects/OPO_LN_Pred_Uncertainty/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/mikedoho/PycharmProjects/OPO_LN_Pred_Uncertainty/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/mikedoho/PycharmProjects/OPO_LN_Pred_Uncertainty/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/mikedoho/PycharmProjects/OPO_LN_Pred_Uncertainty/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13200412843472123930\n",
      "]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Code for hyperparameter optimization using Bayesian methods from\n",
    "# hyperopt library\n",
    "\n",
    "\n",
    "#Setting GPU to use for hyperparameter optimization\n",
    "\n",
    "\n",
    "import os\n",
    "import tensorflow\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "\n",
    "# 0 is 1080ti and 1 is 2080ti \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Versions of libraries\n",
      "numpy version:  1.18.2\n",
      "matplotlib version:  3.2.1\n",
      "sklearn version:  0.22.2.post1\n",
      "keras version:  2.2.4\n",
      "tensorflow version:  1.9.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# importing libraries for keras and tensorflow along with other general libraries\n",
    "\n",
    "import os\n",
    "import scipy.io\n",
    "import h5py\n",
    "import shutil\n",
    "from colorama import Fore, Style, Back\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, precision_recall_fscore_support, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras.engine import Input, Model\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import UpSampling3D, PReLU, Deconvolution3D\n",
    "from keras.layers import Activation, Dense, Input, BatchNormalization\n",
    "from keras.layers import Conv3D, concatenate, MaxPooling3D, Flatten, Dropout\n",
    "from keras.layers import AveragePooling3D, GlobalAveragePooling3D, GlobalMaxPooling3D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils, plot_model\n",
    "\n",
    "# Libraries for hyperopt\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, STATUS_FAIL, tpe, fmin, space_eval, pyll, hp\n",
    "import pickle\n",
    "import traceback\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "try:\n",
    "    from keras.engine import merge\n",
    "except ImportError:\n",
    "    from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "# prevents memory allocation errors when using GPU\n",
    "from keras import backend as K\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "print('Versions of libraries')\n",
    "print('numpy version: ', np.version.version)\n",
    "print('matplotlib version: ', matplotlib.__version__)\n",
    "print('sklearn version: ', sklearn.__version__)\n",
    "print('keras version: ', keras.__version__)\n",
    "print('tensorflow version: ', tf.VERSION)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'Current settings; dont delete'"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('Current settings; dont delete')\n",
    "\n",
    "train_dataset = [1,3,4]\n",
    "valid_dataset = [2]\n",
    "test_dataset = [5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "main_dir = '/home/mikedoho/Desktop/utsw_hn_ai/opo_hn_LN_prediction'\n",
    "model_dir = os.path.join(main_dir, 'Models')\n",
    "\n",
    "data_dir = '/media/hdd_1_2TB/Data/OPSCC_Data/train_val_test_split/'\n",
    "\n",
    "#training data directory\n",
    "data_train_dir = os.path.join(data_dir, 'Train')\n",
    "data_train_ct_dir = os.path.join(data_train_dir, 'CT_Train')\n",
    "data_train_pet_dir = os.path.join(data_train_dir, 'PET_Train')\n",
    "data_train_label_dir = os.path.join(data_train_dir, 'Label_Train')\n",
    "\n",
    "# validation data directory\n",
    "data_val_dir = os.path.join(data_dir, 'Validation')\n",
    "data_val_ct_dir = os.path.join(data_val_dir, 'CT_Val')\n",
    "data_val_pet_dir = os.path.join(data_val_dir, 'PET_Val')\n",
    "data_val_label_dir = os.path.join(data_val_dir, 'Label_Val')\n",
    "\n",
    "# Test data directory\n",
    "data_test_dir = os.path.join(data_dir, 'Test')\n",
    "data_test_ct_dir = os.path.join(data_test_dir, 'CT_Test')\n",
    "data_test_pet_dir = os.path.join(data_test_dir, 'PET_Test')\n",
    "data_test_label_dir = os.path.join(data_test_dir, 'Label_Test')\n",
    "\n",
    "\n",
    "dir_dict = {\n",
    "    'main_dir': main_dir, \n",
    "    'dir_data': data_dir,\n",
    "    \n",
    "    'dir_train_ct': data_train_ct_dir, \n",
    "    'dir_train_pet': data_train_pet_dir,\n",
    "    'dir_train_label': data_train_label_dir,\n",
    "\n",
    "    'dir_val_ct': data_val_ct_dir, \n",
    "    'dir_val_pet': data_val_pet_dir,\n",
    "    'dir_val_label': data_val_label_dir,\n",
    "    \n",
    "    'dir_test_ct': data_test_ct_dir, \n",
    "    'dir_test_pet': data_test_pet_dir,\n",
    "    'dir_test_label': data_test_label_dir,\n",
    "    \n",
    "    'dir_model': model_dir\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def load_train_data_batch_generator(batch_size=32, rows_in=72, cols_in=72, zs_in=48, \n",
    "                                    channels_in=2, num_classes=2, \n",
    "                                    dir_dict=dir_dict):\n",
    "    '''\n",
    "    Training data generator\n",
    "    batch_size: batch size\n",
    "    rows_in: # rows (pixels) in image\n",
    "    cols_in: # cols (pixels) in image\n",
    "    zs_in: # images in CT/PET\n",
    "    channels_in: # of image modalities (CT and PET)\n",
    "    num_classes: # of classes that model is predicting\n",
    "    dir_dict: dictionary containing directories \n",
    "    '''  \n",
    "       \n",
    "    # required when using hyperopt\n",
    "    batch_size = int(batch_size)\n",
    "    # if not: TypeError: 'float' object cannot be interpreted as an integer\n",
    "    \n",
    "    # names of all files in directory\n",
    "    fnames = os.listdir(dir_dict['dir_train_ct'])\n",
    "    \n",
    "    y_train = np.zeros((batch_size, num_classes))\n",
    "    x_train = np.zeros((batch_size, rows_in, cols_in, zs_in, channels_in))\n",
    "\n",
    "    while True:\n",
    "        count = 0\n",
    "        for fname in np.random.choice(fnames, batch_size, replace=False):\n",
    "\n",
    "            data_label = scipy.io.loadmat(os.path.join(dir_dict['dir_train_label'], fname), \n",
    "                                          verify_compressed_data_integrity=False)['label']\n",
    "            \n",
    "#             data_label = data_label[0]\n",
    "\n",
    "            # changing one hot encoding to integer\n",
    "            integer_label = np.argmax(data_label[0], axis=0)\n",
    "            y_train[count,:] = data_label\n",
    "#             print(fname)\n",
    "            \n",
    "\n",
    "            # Loading train ct w/ c and pet/ct combo \n",
    "            train_ct = scipy.io.loadmat(os.path.join(dir_dict['dir_train_ct'], fname),\n",
    "                                        verify_compressed_data_integrity=False)['roi_patch_ct']\n",
    "            train_pet = scipy.io.loadmat(os.path.join(dir_dict['dir_train_pet'], fname), \n",
    "                                         verify_compressed_data_integrity=False)['roi_patch_pet']\n",
    "            \n",
    "            x_train[..., 0] = train_ct/1\n",
    "            x_train[..., 1] = train_pet/14.30099\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        yield(x_train, y_train)\n",
    "\n",
    "\n",
    "def load_val_data_batch_generator(batch_size=32, rows_in=72, cols_in=72, zs_in=48, \n",
    "                                  channels_in=2, num_classes=2, \n",
    "                                  dir_dict=dir_dict):\n",
    "    '''\n",
    "    Valdiation data generator\n",
    "    batch_size: batch size\n",
    "    rows_in: # rows (pixels) in image\n",
    "    cols_in: # cols (pixels) in image\n",
    "    zs_in: # images in CT/PET\n",
    "    channels_in: # of image modalities (CT and PET)\n",
    "    num_classes: # of classes that model is predicting\n",
    "    dir_dict: dictionary containing directories \n",
    "    '''  \n",
    "    \n",
    "    # required when using hyperopt\n",
    "    batch_size = int(batch_size)\n",
    "    # if not: TypeError: 'float' object cannot be interpreted as an integer\n",
    "\n",
    "    # names of all files in directory \n",
    "    fnames = os.listdir(dir_dict['dir_val_ct'])\n",
    "    \n",
    "    y_val = np.zeros((batch_size, num_classes))\n",
    "    x_val = np.zeros((batch_size, rows_in, cols_in, zs_in, channels_in))\n",
    "\n",
    "    while True:\n",
    "        count = 0\n",
    "        for fname in np.random.choice(fnames, batch_size, replace=False):\n",
    "\n",
    "            data_label = scipy.io.loadmat(os.path.join(dir_dict['dir_val_label'], fname), \n",
    "                                          verify_compressed_data_integrity=False)['label']\n",
    "\n",
    "            # changing one hot encoding to integer\n",
    "            integer_label = np.argmax(data_label[0], axis=0)\n",
    "            y_val[count,:] = data_label\n",
    "\n",
    "            # Loading val ct w/ c and pet/ct combo \n",
    "            val_ct = scipy.io.loadmat(os.path.join(dir_dict['dir_val_ct'], fname), \n",
    "                                      verify_compressed_data_integrity=False)['roi_patch_ct']\n",
    "            val_pet = scipy.io.loadmat(os.path.join(dir_dict['dir_val_pet'], fname), \n",
    "                                       verify_compressed_data_integrity=False)['roi_patch_pet']\n",
    "            \n",
    "            x_val[..., 0] = val_ct/1\n",
    "            x_val[..., 1] = val_pet/14.30099\n",
    "\n",
    "            count += 1\n",
    "                       \n",
    "        yield(x_val, y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def load_train_data_batch(batch_size=32, rows_in=72, cols_in=72, zs_in=48, \n",
    "                          channels_in=2, num_classes=2, \n",
    "                          dir_dict=dir_dict):\n",
    "    '''\n",
    "    Training data loader for AUC curve\n",
    "    batch_size: batch size\n",
    "    rows_in: # rows (pixels) in image\n",
    "    cols_in: # cols (pixels) in image\n",
    "    zs_in: # images in CT/PET\n",
    "    channels_in: # of image modalities (CT and PET)\n",
    "    num_classes: # of classes that model is predicting\n",
    "    dir_dict: dictionary containing directories \n",
    "    '''  \n",
    "    \n",
    "    # required when using hyperopt\n",
    "    batch_size = int(batch_size)\n",
    "    # if not: TypeError: 'float' object cannot be interpreted as an integer\n",
    "    \n",
    "    fnames = os.listdir(dir_dict['dir_train_ct'])\n",
    "    \n",
    "    y_train = np.zeros((batch_size, num_classes))\n",
    "    x_train = np.zeros((batch_size, rows_in, cols_in, zs_in, channels_in))\n",
    "\n",
    "    while True:\n",
    "        count = 0\n",
    "        for fname in np.random.choice(fnames, batch_size, replace=False):\n",
    "\n",
    "            data_label = scipy.io.loadmat(os.path.join(dir_dict['dir_train_label'], fname), \n",
    "                                          verify_compressed_data_integrity=False)['label']\n",
    "\n",
    "            # changing one hot encoding to integer\n",
    "            integer_label = np.argmax(data_label[0], axis=0)\n",
    "            y_train[count,:] = data_label\n",
    "\n",
    "            # Loading train ct w/ c and pet/ct combo \n",
    "            train_ct = scipy.io.loadmat(os.path.join(dir_dict['dir_train_ct'], fname),\n",
    "                                        verify_compressed_data_integrity=False)['roi_patch_ct']\n",
    "            train_pet = scipy.io.loadmat(os.path.join(dir_dict['dir_train_pet'], fname), \n",
    "                                         verify_compressed_data_integrity=False)['roi_patch_pet']\n",
    "            \n",
    "            x_train[..., 0] = train_ct/1\n",
    "            x_train[..., 1] = train_pet/14.30099\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        return(x_train, y_train)\n",
    "\n",
    "\n",
    "def load_val_data_batch(batch_size=32, rows_in=72, cols_in=72, zs_in=48, \n",
    "                        channels_in=2, num_classes=2, \n",
    "                        dir_dict=dir_dict):\n",
    "    '''\n",
    "    Validation data loader for AUC curve\n",
    "    batch_size: batch size\n",
    "    rows_in: # rows (pixels) in image\n",
    "    cols_in: # cols (pixels) in image\n",
    "    zs_in: # images in CT/PET\n",
    "    channels_in: # of image modalities (CT and PET)\n",
    "    num_classes: # of classes that model is predicting\n",
    "    dir_dict: dictionary containing directories \n",
    "    '''  \n",
    "    \n",
    "    # required when using hyperopt\n",
    "    batch_size = int(batch_size)\n",
    "    # if not: TypeError: 'float' object cannot be interpreted as an integer\n",
    "\n",
    "    fnames = os.listdir(dir_dict['dir_val_ct'])\n",
    "    \n",
    "    y_val = np.zeros((batch_size, num_classes))\n",
    "    x_val = np.zeros((batch_size, rows_in, cols_in, zs_in, channels_in))\n",
    "\n",
    "    while True:\n",
    "        count = 0\n",
    "        for fname in np.random.choice(fnames, batch_size, replace=False):\n",
    "\n",
    "            data_label = scipy.io.loadmat(os.path.join(dir_dict['dir_val_label'], fname), \n",
    "                                          verify_compressed_data_integrity=False)['label']\n",
    "\n",
    "            # changing one hot encoding to integer\n",
    "            integer_label = np.argmax(data_label[0], axis=0)\n",
    "            y_val[count,:] = data_label\n",
    "\n",
    "            # Loading val ct w/ c and pet/ct combo \n",
    "            val_ct = scipy.io.loadmat(os.path.join(dir_dict['dir_val_ct'], fname), \n",
    "                                      verify_compressed_data_integrity=False )['roi_patch_ct']\n",
    "            val_pet = scipy.io.loadmat(os.path.join(dir_dict['dir_val_pet'], fname),\n",
    "                                       verify_compressed_data_integrity=False)['roi_patch_pet']\n",
    "            \n",
    "            x_val[..., 0] = val_ct/1\n",
    "            x_val[..., 1] = val_pet/14.30099\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        return(x_val, y_val)\n",
    "    \n",
    "def load_test_data_batch(batch_size=32, rows_in=72, cols_in=72, zs_in=48, \n",
    "                        channels_in=2, num_classes=2, \n",
    "                        dir_dict=dir_dict):\n",
    "    '''\n",
    "    Validation data loader for AUC curve\n",
    "    batch_size: batch size\n",
    "    rows_in: # rows (pixels) in image\n",
    "    cols_in: # cols (pixels) in image\n",
    "    zs_in: # images in CT/PET\n",
    "    channels_in: # of image modalities (CT and PET)\n",
    "    num_classes: # of classes that model is predicting\n",
    "    dir_dict: dictionary containing directories \n",
    "    '''  \n",
    "    \n",
    "    # required when using hyperopt\n",
    "    batch_size = int(batch_size)\n",
    "    # if not: TypeError: 'float' object cannot be interpreted as an integer\n",
    "\n",
    "    fnames = os.listdir(dir_dict['dir_test_ct'])\n",
    "    \n",
    "    y_test = np.zeros((batch_size, num_classes))\n",
    "    x_test = np.zeros((batch_size, rows_in, cols_in, zs_in, channels_in))\n",
    "\n",
    "    while True:\n",
    "        count = 0\n",
    "        for fname in np.random.choice(fnames, batch_size, replace=False):\n",
    "\n",
    "            data_label = scipy.io.loadmat(os.path.join(dir_dict['dir_test_label'], fname),\n",
    "                                          verify_compressed_data_integrity=False)['label']\n",
    "\n",
    "            # changing one hot encoding to integer\n",
    "            integer_label = np.argmax(data_label[0], axis=0)\n",
    "            y_test[count,:] = data_label\n",
    "\n",
    "            # Loading val ct w/ c and pet/ct combo \n",
    "            val_ct = scipy.io.loadmat(os.path.join(dir_dict['dir_test_ct'], fname), \n",
    "                                      verify_compressed_data_integrity=False)['roi_patch_ct']\n",
    "            val_pet = scipy.io.loadmat(os.path.join(dir_dict['dir_test_pet'], fname), \n",
    "                                       verify_compressed_data_integrity=False)['roi_patch_pet']\n",
    "            \n",
    "            x_test[..., 0] = val_ct/1\n",
    "            x_test[..., 1] = val_pet/14.30099\n",
    "\n",
    "            count += 1\n",
    "\n",
    "        return(x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "'''\n",
    "Hyper parameter space for UNET model \n",
    "'''\n",
    "\n",
    "unet_hype_space = {\n",
    "    \n",
    "    'nb_classes': 2,\n",
    "    'input_shape': np.shape(np.zeros((72, 72, 48, 2))),\n",
    "    'batch_size': hp.quniform('batch_size', 10, 20, 2),\n",
    "    'epochs': hp.choice('epochs_mike', [50]),\n",
    "    'class_weights': hp.choice('class_weights', [1, 2, 3]),\n",
    "    \n",
    "    'learning_rate': hp.choice('learning_rate', [1e-5, 5*1e-5, 1e-4]),\n",
    "    \n",
    "    'conv_dropout_drop_proba': hp.choice('conv_dropout_proba', [0.01, 0.015]),   \n",
    "    'fc_dropout_drop_proba': hp.choice('fc_dropout_proba', [0.1, 0.2]),\n",
    "    \n",
    "    'final_activation': hp.choice('final_activation',[\"softmax\"]),\n",
    "    'metrics': hp.choice('metrics', ['accuracy']),\n",
    "    'loss': hp.choice('loss',['categorical_crossentropy']),\n",
    "    \n",
    "    # Use batch normalisation at more places?\n",
    "    'use_BN': hp.choice('use_BN', [False, True]),\n",
    "    'use_DO_conv': hp.choice('use_DO_conv', [True]),\n",
    "    'use_DO_dense': hp.choice('use_DO_dense', [True]),\n",
    "            \n",
    "    'MP_stride': hp.choice('MP_stride_unet', [(2,2,2), (3,3,3)]),\n",
    "    'MP_pool': hp.choice('MP_pool_unet', [(3,3,3), (2,2,2)]),\n",
    "      \n",
    "    # UNET specific\n",
    "    'UNET_depth': hp.choice('UNET_depth', [1, 2, 3]),\n",
    "    'kernel_size': hp.choice('kernel_size', [(2,2,2), (3,3,3)]),\n",
    "    'padding': hp.choice('padding', ['valid']),\n",
    "    'n_filters': hp.choice('n_filters', [20, 30, 15]), \n",
    "    'use_REG': hp.choice('use_REG', [False, True]),\n",
    "    'l2_reg': hp.loguniform('l2_reg', -3, 3),\n",
    "#     'l1_reg': hp.loguniform('l1_reg', -2, 2),\n",
    "    'instance_normalization': hp.choice('instance_normalization', [False, True]),\n",
    "    'deconvolution': hp.choice('deconvolution', [True]),\n",
    "    'n_first_dense_filters': hp.choice('n_first_dense_filters', [10, 20, 40, 60]),\n",
    "    'conv_activation': hp.choice('conv_activation', ['relu']),\n",
    "    \n",
    "    'which_model': 'unet'\n",
    "    \n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def unet_model_3d(space):\n",
    "    \"\"\"\n",
    "    Builds the 3D UNet Keras model.f\n",
    "    :param metrics: List metrics to be calculated during model training.\n",
    "    :param n_filters: The number of filters that the first layer in the convolution network will have. Following\n",
    "    layers will contain a multiple of this number. Lowering this number will likely reduce the amount of memory required\n",
    "    to train the model.\n",
    "    :param UNET_depth: indicates the depth of the U-shape for the model. The greater the depth, the more max pooling\n",
    "    layers will be added to the model. Lowering the depth may reduce the amount of memory required for training.\n",
    "    :param input_shape: Shape of the input data (x_size, y_size, z_size, n_channels). The x, y, and z sizes must be\n",
    "    divisible by the pool size to the power of the depth of the UNet, that is pool_size^depth.\n",
    "    :param pool_size: Pool size for the max pooling operations.\n",
    "    :param num_classes: Number of binary labels that the model is learning.\n",
    "    :param deconvolution: If set to True, will use transpose convolution(deconvolution) instead of up-sampling. This\n",
    "    increases the amount memory required during training.\n",
    "    :return: Untrained 3D UNet Model\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Creating Unet...')\n",
    "    print('Filter size output is truncated')\n",
    "    \n",
    "    inputs = Input(space['input_shape'])\n",
    "    current_layer = inputs\n",
    "    levels = list()\n",
    "\n",
    "    # add levels with max pooling\n",
    "    for layer_depth in range(space['UNET_depth']):\n",
    "        layer1 = create_convolution_block(space=space,\n",
    "                                          input_layer=current_layer, \n",
    "                                          n_filters=space['n_filters']*(2**layer_depth))\n",
    "        \n",
    "        layer2 = create_convolution_block(space=space,\n",
    "                                          input_layer=layer1, \n",
    "                                          n_filters=space['n_filters']*(2**layer_depth)*2)\n",
    "        \n",
    "        if layer_depth < space['UNET_depth'] - 1:\n",
    "            current_layer = MaxPooling3D(pool_size=space['MP_pool'])(layer2)\n",
    "            levels.append([layer1, layer2, current_layer])\n",
    "        else:\n",
    "            current_layer = layer2\n",
    "            levels.append([layer1, layer2])\n",
    "\n",
    "    # add levels with up-convolution or up-sampling\n",
    "    for layer_depth in range(space['UNET_depth']-2, -1, -1):\n",
    "        up_convolution = get_up_convolution(pool_size=space['MP_pool'], \n",
    "                                            deconvolution=space['deconvolution'],\n",
    "                                            n_filters=current_layer._keras_shape[-1])(current_layer)\n",
    "     \n",
    "        concat = concatenate([up_convolution, levels[layer_depth][1]], axis=-1)\n",
    "        \n",
    "        current_layer = create_convolution_block(space=space,\n",
    "                                                 n_filters=levels[layer_depth][1]._keras_shape[-1],\n",
    "                                                 input_layer=concat)\n",
    "        \n",
    "        current_layer = create_convolution_block(space=space,\n",
    "                                                 n_filters=levels[layer_depth][1]._keras_shape[-1],\n",
    "                                                 input_layer=current_layer)\n",
    "\n",
    "    final_convolution = Conv3D(space['nb_classes'], (1, 1, 1))(current_layer)\n",
    "    act = Activation(space['conv_activation'])(final_convolution)\n",
    "    \n",
    "    flatten = Flatten()(act)\n",
    "    dense1 = Dense(space['n_first_dense_filters'], activation='relu')(flatten)\n",
    "    \n",
    "    if space['use_DO_dense']:\n",
    "        dense1 = Dropout(space['fc_dropout_drop_proba'])(dense1)\n",
    "    \n",
    "    dense2 = Dense(space['nb_classes'], \n",
    "                   activation=space['final_activation'], name='predictions')(dense1)\n",
    "    \n",
    "    #print('Proof of filter size: {}'.format(current_layer._keras_shape[-1]))\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=dense2)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=space['learning_rate']*space['lr_rate_mult']),\n",
    "                 loss=space['loss'],\n",
    "                 metrics=[space['metrics']])\n",
    "    \n",
    "    \n",
    "#     print('Plotting model...')\n",
    "#     plot_model(model, to_file='unet_model.png')\n",
    "#     print('Picture made. Please see in ', os.getcwd())\n",
    "#     print('\\nCurrent parameter space \\n',space)\n",
    "#     print('\\n')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_convolution_block(space,\n",
    "                             input_layer, \n",
    "                             n_filters):\n",
    "    \"\"\"\n",
    "    Used in creating UNET\n",
    "    :param strides:\n",
    "    :param input_layer:\n",
    "    :param n_filters:\n",
    "    :param batch_normalization:\n",
    "    :param kernel:\n",
    "    :param activation: Keras activation layer to use. (default is 'relu')\n",
    "    :param padding:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    if space['use_REG']:\n",
    "        layer = Conv3D(filters=space['n_filters'], \n",
    "                       kernel_size=space['kernel_size'], \n",
    "                       padding=space['padding'], \n",
    "                       strides=space['MP_stride'],\n",
    "                       kernel_regularizer=regularizers.l2(space['l2_reg']))(input_layer)\n",
    "        \n",
    "    else: layer = Conv3D(filters=space['n_filters'], \n",
    "                         kernel_size=space['kernel_size'], \n",
    "                         padding=space['padding'], \n",
    "                         strides=space['MP_stride'])(input_layer)\n",
    "    \n",
    "    \n",
    "    if space['use_BN']:\n",
    "        layer = BatchNormalization(axis=-1)(layer)\n",
    "        \n",
    "    if space['use_DO_conv']:\n",
    "        layer = Dropout(space['conv_dropout_drop_proba'])(layer)    \n",
    "    \n",
    "    act = Activation(space['conv_activation'])(layer)\n",
    "    \n",
    "    return act\n",
    "\n",
    "def get_up_convolution(n_filters, pool_size, kernel_size=(2, 2, 2), strides=(2, 2, 2),\n",
    "                       deconvolution=False):\n",
    "    ''' Used in creating UNET'''\n",
    "    \n",
    "    if deconvolution:\n",
    "        return Deconvolution3D(filters=n_filters, kernel_size=kernel_size,\n",
    "                               strides=strides)\n",
    "    else:\n",
    "        return UpSampling3D(size=pool_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def roc_auc_plot(y_true, y_pred, n_classes=2, lw=2, data_title='Unspecified'):\n",
    "    '''\n",
    "    plot ROC and compute auc using sklearn. Taken from \n",
    "    https://github.com/Tony607/ROC-Keras/blob/master/ROC-Keras.ipynb.\n",
    "    #y_test: actual results\n",
    "    #y_score: predicted results\n",
    "    #n_classes: number of classes\n",
    "    #lw: linewidth\n",
    "    '''\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    # Plot all ROC curves\n",
    "    plt.figure(1)\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                 ''.format(i, roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC--{data_title}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def dr_friendly_measures(y_true, y_pred):\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    specificity = tn/(tn+fp)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    ppv = tp/(tp+fp)\n",
    "    npv = tn/(tn+fn)\n",
    "    \n",
    "    name1 = ['specificity', 'sensitivity', 'ppv', 'npv']\n",
    "    name2 = [specificity, sensitivity, ppv, npv]\n",
    "    \n",
    "    for i in range(len(name1)):\n",
    "        print(f\"{name1[i]}: {np.round(name2[i], 3)}\")\n",
    "    \n",
    "    return specificity, sensitivity, ppv, npv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train_model(space,\n",
    "                dir_dict=dir_dict, \n",
    "                plot_vis=True, \n",
    "                auroc=True,\n",
    "                data_vis=False,\n",
    "                gen_compare=False,\n",
    "                verbose=1):\n",
    "    '''\n",
    "    Trains the model\n",
    "    Callbacks include EarlyStopping and ReduceLROnPlateau\n",
    "    ROC curves and precision, recall, fscore calculate for validation and test data\n",
    "    Current model is compared to saved model then saves current model if better\n",
    "    Calculates specified value for hyperopt minimization\n",
    "    '''\n",
    "        \n",
    "    model = unet_model_3d(space)\n",
    "   \n",
    "    \n",
    "    call_back = [EarlyStopping(monitor='val_loss', patience=40, min_delta=0.005,\n",
    "                               restore_best_weights=True, verbose=verbose),\n",
    "                 ReduceLROnPlateau(monitor='val_loss', factor=0.8,\n",
    "                              patience=20, min_lr=5e-8,  \n",
    "                              verbose=verbose)]\n",
    "    \n",
    "    print('Evaluating parameter space\\n')\n",
    "    \n",
    "    display(round(len(os.listdir(dir_dict['dir_val_label']))/space['batch_size']))\n",
    "    display(len(os.listdir(dir_dict['dir_val_label'])))\n",
    "    display(space['batch_size'])\n",
    "    \n",
    "    # History\n",
    "    history = model.fit_generator(\n",
    "            generator=load_train_data_batch_generator(batch_size=space['batch_size']),\n",
    "            steps_per_epoch=round(len(os.listdir(dir_dict['dir_train_ct']))/space['batch_size']),\n",
    "            epochs=space['epochs'],\n",
    "            verbose=verbose,\n",
    "            callbacks=call_back,\n",
    "            validation_data=load_val_data_batch_generator(batch_size=space['batch_size']),\n",
    "            validation_steps=round(len(os.listdir(dir_dict['dir_val_ct']))/space['batch_size']),\n",
    "            class_weight={0:1, 1:3}\n",
    "    ).history\n",
    "    \n",
    "    \n",
    "    # Test model to obtain score/acc:\n",
    "    \n",
    "    \n",
    "    score = model.evaluate_generator(generator = load_val_data_batch_generator(batch_size=space['batch_size']), \n",
    "                                     steps = round(len(os.listdir(dir_dict['dir_val_label']))/space['batch_size']))\n",
    "    \n",
    "        \n",
    "    if auroc:\n",
    "        # Creating ROC curve for validation data\n",
    "        val_x_auc, val_y_auc =  load_val_data_batch(len(os.listdir(dir_dict['dir_val_label'])))\n",
    "        y_pred = model.predict(x=val_x_auc)\n",
    "        roc_auc_plot(val_y_auc, y_pred, data_title='VALIDATION')\n",
    "        \n",
    "        \n",
    "        target_names = ['class '+str(x) for x in range(space['nb_classes'])]\n",
    "        \n",
    "        print('VALIDATION INFORMATION')\n",
    "        \n",
    "        # printing precision, recall, and fscore for each class for validation data\n",
    "        print('\\n', classification_report(np.argmax(val_y_auc, axis=-1), \n",
    "                                          np.argmax(y_pred, axis=-1), \n",
    "                                          target_names=target_names))     \n",
    "        \n",
    "        \n",
    "        def weight_avg(x, y):\n",
    "            '''Compute weighted average'''\n",
    "            numarator = []\n",
    "            numarator = [x[i]*y[i]for i in range(len(x))]\n",
    "            num_sum = sum(numarator)\n",
    "            weight_avg = num_sum/sum(y)\n",
    "\n",
    "            return weight_avg\n",
    "    \n",
    "        \n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(np.argmax(val_y_auc, axis=-1), \n",
    "                                                                                 np.argmax(y_pred, axis=-1))\n",
    "\n",
    "        specificity, sensitivity, ppv, npv = dr_friendly_measures(np.argmax(val_y_auc, axis=-1), \n",
    "                                                                      np.argmax(y_pred, axis=-1))\n",
    "        \n",
    "        # being used as metrics for optimization in hyperopt *min(-metric)\n",
    "        avg_fscore = weight_avg(fscore, support)\n",
    "        avg_fscore_metric = weight_avg(fscore, support)\n",
    "        \n",
    "        # focus scores to be disproportionally  worse or better\n",
    "        if sensitivity == 0 or specificity == 0:\n",
    "            hyper_opt_score = 1\n",
    "        elif sensitivity >= 0.85 and specificity >=0.85 and sensitivity > specificity:\n",
    "            hyper_opt_score = 4.5\n",
    "        elif sensitivity >= 0.85 and specificity >=0.85: \n",
    "            hyper_opt_score = 4.3\n",
    "        elif sensitivity >= 0.8 and specificity >=0.8 and sensitivity > specificity:\n",
    "            hyper_opt_score = 4.2\n",
    "        elif sensitivity >= 0.8 and specificity >=0.8:\n",
    "            hyper_opt_score = 4\n",
    "        else:\n",
    "            hyper_opt_score = (3*sensitivity + specificity)\n",
    "        \n",
    "        eval_val_metric = [precision, recall, fscore, avg_fscore]\n",
    "        eval_val_metric_dr = [specificity, sensitivity, ppv, npv]\n",
    "               \n",
    "\n",
    "        # Process repeated above but to see test data \n",
    "        test_x_auc, test_y_auc =  load_test_data_batch(len(os.listdir(dir_dict['dir_test_label'])))\n",
    "        y_pred_test = model.predict(x=test_x_auc)\n",
    "        roc_auc_plot(test_y_auc, y_pred_test, data_title='TEST')\n",
    "        \n",
    "        \n",
    "        print('TEST INFORMATION')\n",
    "        \n",
    "        # printing precision, recall, and fscore for each class for test data\n",
    "        print('\\n', classification_report(np.argmax(test_y_auc, axis=-1), \n",
    "                                          np.argmax(y_pred_test, axis=-1), \n",
    "                                          target_names=target_names))\n",
    "        \n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(np.argmax(test_y_auc, axis=-1), \n",
    "                                                                             np.argmax(y_pred_test, axis=-1))\n",
    "        \n",
    "        dr_friendly_measures(np.argmax(test_y_auc, axis=-1), \n",
    "                                       np.argmax(y_pred_test, axis=-1))\n",
    "    \n",
    "    max_acc = max(history['val_acc'])\n",
    "    \n",
    "    if data_vis:\n",
    "        # obtain numeric representation of model evaluation\n",
    "        print('\\n')\n",
    "        print(history.keys())\n",
    "        print(history)\n",
    "        print(model.metrics_names)\n",
    "        print(score)\n",
    "        print('\\n\\n')\n",
    "    \n",
    "    if plot_vis:\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.plot(history['val_acc'])\n",
    "        plt.plot(history['acc'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['val_acc', \n",
    "                    'acc'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        try:\n",
    "            history['val_sk_auroc']\n",
    "        except KeyError:\n",
    "            print('no val_sk_auroc')\n",
    "        else:\n",
    "            #Plot training & validation auc/sk_auc\n",
    "            plt.plot(history['val_sk_auroc'])\n",
    "            plt.plot(history['sk_auroc'])\n",
    "    #         plt.plot(history['val_auc'])\n",
    "    #         plt.plot(history['auc'])\n",
    "            plt.title('Model AUC/sk_AUC')\n",
    "            plt.ylabel('AUC')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(['val_sk_auc', 'sk_auc'], loc='upper left')\n",
    "            plt.show()\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.plot(history['val_loss'])\n",
    "        plt.plot(history['loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['val_loss', 'loss'], loc='upper left')\n",
    "        plt.show()\n",
    "    \n",
    "    results = {\n",
    "        # We plug \"-hyper_opt_score\" as a\n",
    "        # minimizing metric named 'loss' by Hyperopt.\n",
    "        'loss': -hyper_opt_score,\n",
    "        'real_loss': score[0],\n",
    "        # stats:\n",
    "        'best_loss': min(history['val_loss']),\n",
    "        'best_accuracy': max(history['val_acc']),\n",
    "        'end_loss': score[0],\n",
    "        'end_accuracy': score[1],\n",
    "        # Misc:\n",
    "        'space': space,\n",
    "        'history': history,\n",
    "        'status': STATUS_OK\n",
    "    }\n",
    "    \n",
    "\n",
    "    return model, results, eval_val_metric, eval_val_metric_dr, space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#LIYUAN THIS IS THE CODE SPECIFIC TO HYPEROPT\n",
    "#YOU WILL SEE THAT THERE IS CODE THAT USES PICKLE TO SAVE THE HYPERPARAMETERS SAVED BY HYPEROPT\n",
    "#I EDITED THE CODE TO MAKE IT MORE BARE BONES. MIGHT WANT TO ADD LINE THAT SAVES MODEL\n",
    "\n",
    "def optimize_cnn(space):\n",
    "    \"\"\"Build a convolutional neural network and train it for optimization.\"\"\"\n",
    "    \n",
    "    # need to delete \"model\" so dict sequence can be updated\n",
    "    \n",
    "    try:\n",
    "        model, results, x, y, z = train_model(space)\n",
    "        \n",
    "        print(f\"\\nHyperopt min: {results['loss']}\\n\")\n",
    "        \n",
    "        #COULD PUT CODE THAT SAVES CURRENT MODEL; THOUGH IT WILL NOT BE WITH HYPERPARAMETERS SELECTED BY\n",
    "        #HYPEROPT\n",
    "\n",
    "        K.clear_session()\n",
    "        \n",
    "        del model, x, y, z\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as err:\n",
    "        try:\n",
    "            K.clear_session()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #this really isnt needed in jupyter notebook\n",
    "        err_str = str(err)\n",
    "        print(err_str)\n",
    "        traceback_str = str(traceback.format_exc())\n",
    "        print(traceback_str)\n",
    "        return {\n",
    "            'status': STATUS_FAIL,\n",
    "            'err': err_str,\n",
    "            'traceback': traceback_str\n",
    "        }\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def run_trial(space, save_model_str):\n",
    "    \"\"\"Run one TPE meta optimization step and save its results.\"\"\"\n",
    "    max_evals = nb_evals = 1\n",
    " \n",
    "\n",
    "    print(\"Attempt to resume a past training if it exists:\")\n",
    "    os.chdir('/home/mikedoho/Desktop/utsw_hn_ai/opo_hn_LN_prediction/Model_Trial_Results')\n",
    "\n",
    "    try:\n",
    "        # https://github.com/hyperopt/hyperopt/issues/267\n",
    "        \n",
    "        print('\\nEvaluating using hyperparamers from {}_hype_space'.format(space['which_model']))\n",
    "        print('Loading {} Model from '.format(space['which_model'].capitalize()), os.getcwd())\n",
    "        trials = pickle.load(open(f\"results_{space['which_model']}_{save_model_str}.pkl\", 'rb'))\n",
    "        print('Model loaded!\\n')\n",
    "        \n",
    "        #print(\"Found saved Trials! Loading...\")\n",
    "        max_evals = len(trials.trials) + nb_evals\n",
    "        print(\"Rerunning from {} trials to add another one.\".format(\n",
    "            len(trials.trials)))\n",
    "\n",
    "    \n",
    "    except:\n",
    "        trials = Trials()\n",
    "        print(\"Starting from scratch: new trials.\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    best = fmin(\n",
    "                fn=optimize_cnn,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                trials=trials,\n",
    "                max_evals=max_evals\n",
    "    )\n",
    "    \n",
    "    #Save Parameter space for Cross Validations\n",
    "#     os.chdir('/home/mikedoho/Desktop/utsw_hn_ai/opo_hn_LN_prediction/Parameter_Space_CV')\n",
    "#     with open(f\"Parameter_space_CV\", 'wb') as f:\n",
    "#         pickle.dump(trials.trials[-1]['result']['space'], f)\n",
    "    \n",
    "    os.chdir(f\"/home/mikedoho/Desktop/utsw_hn_ai/opo_hn_LN_prediction/Models/Best_Hyperopt_Models/{space['which_model']}\")\n",
    "    \n",
    "    with open(f\"Best_{space['which_model']}_Model_{save_model_str}\", 'wb') as f:\n",
    "        pickle.dump(space_eval(space, best), f)\n",
    "    \n",
    "    #This is a line of code that will need to change if reused\n",
    "    os.chdir('/home/mikedoho/Desktop/utsw_hn_ai/opo_hn_LN_prediction/Model_Trial_Results')\n",
    "    \n",
    "    print('Saving {} Model to '.format(space['which_model'].capitalize()), os.getcwd())\n",
    "    trials = pickle.dump(trials, open(f\"results_{space['which_model']}_{save_model_str}.pkl\", 'wb'))\n",
    "    print('Model saved!')\n",
    "    \n",
    "    print(\"\\nOPTIMIZATION STEP COMPLETE.\\n\")\n",
    "    print('\\nCurrent best:\\n',space_eval(space, best),'\\n')\n",
    "   \n",
    "    return best, space_eval(space, best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "save_model_str='postmanu_3sen_spec_1'\n",
    "space = unet_hype_space\n",
    "\n",
    "while True:\n",
    "    \n",
    "\n",
    "    try:\n",
    "        best, num_space = run_trial(space=space, \n",
    "                                save_model_str=save_model_str)\n",
    "        \n",
    "    except: pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}